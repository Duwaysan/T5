{"cells":[{"cell_type":"code","execution_count":null,"id":"AD8_Pz0ZUF8x","metadata":{"id":"AD8_Pz0ZUF8x"},"outputs":[],"source":["# import zipfile\n","# import os\n","\n","# # Define the path to the uploaded zip file and the extraction directory\n","# zip_file_path = '/content/drive/MyDrive/Datasets/Copy of CatsVsDogs.zip'\n","# extraction_dir = '/content/drive/MyDrive/Datasets/DogsVSCats'\n","\n","# # Unzip the file\n","# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","#     zip_ref.extractall(extraction_dir)\n","\n","# # Verify the extraction\n","# print(\"Extracted files:\")\n","# print(os.listdir(extraction_dir))"]},{"cell_type":"markdown","id":"7a07a343","metadata":{"id":"7a07a343"},"source":["# Practice Exercise on Convolutional Neural Networks (CNN)\n","\n","Welcome to the Practice Exercise on Convolutional Neural Networks (CNN). In this exercise, we will focus on an image classification task where the goal is to predict whether an image contains a cat or a dog. We will work with a dataset of labeled images and build, train, and evaluate a CNN model. This practice will allow you to apply your understanding of CNNs to achieve high accuracy in image classification.\n","\n","---\n","\n","## Dataset Overview\n","\n","### **Dataset Name:** Cats and Dogs Image Dataset\n","\n","### **Description:**  \n","The dataset contains images of cats and dogs labeled for classification purposes. Each image belongs to one of the two classes: 'Cat' or 'Dog'. The goal is to classify the images correctly based on the content (i.e., whether the image is of a cat or a dog). The dataset is often used to test image classification models.\n","\n","### **Features:**\n","There are two main folders which are:\n","- `Cat`: Images labeled as containing a cat.\n","- `Dog`: Images labeled as containing a dog.\n","\n","### **Target Variable:**\n","- The goal is to predict whether an image contains a cat or a dog.\n"]},{"cell_type":"markdown","id":"3cb05577","metadata":{"id":"3cb05577"},"source":["## Data Loading and Preprocessing"]},{"cell_type":"markdown","id":"fa968f9e","metadata":{"id":"fa968f9e"},"source":["\n","We will start by loading the dataset and preprocessing the images. This includes:\n","- Resizing images .\n","- Normalizing pixel values.\n","\n","Add more if needed!\n"]},{"cell_type":"code","execution_count":1,"id":"8XszH_sTTSk6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2860,"status":"ok","timestamp":1723984091035,"user":{"displayName":"Khaled","userId":"10230634685242631763"},"user_tz":-180},"id":"8XszH_sTTSk6","outputId":"3bf030f9-096c-4201-b462-96b5d19fb603"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"id":"sbMVcorBTNTc","metadata":{"executionInfo":{"elapsed":4274,"status":"ok","timestamp":1723984096052,"user":{"displayName":"Khaled","userId":"10230634685242631763"},"user_tz":-180},"id":"sbMVcorBTNTc"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import os\n","from glob import glob"]},{"cell_type":"code","execution_count":3,"id":"qaHitSnvTNRH","metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1723984096053,"user":{"displayName":"Khaled","userId":"10230634685242631763"},"user_tz":-180},"id":"qaHitSnvTNRH"},"outputs":[],"source":["dir = '/content/drive/MyDrive/Datasets/DogsVSCats/content/PetImages/'\n","dog_dir = os.path.join(dir, 'Dog')\n","cat_dir = os.path.join(dir, 'Cat')\n"]},{"cell_type":"code","execution_count":null,"id":"ZSM-kACufhAe","metadata":{"id":"ZSM-kACufhAe"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"CrDHy6YRnmdr","metadata":{"executionInfo":{"elapsed":9422,"status":"ok","timestamp":1723972137392,"user":{"displayName":"Khaled","userId":"10230634685242631763"},"user_tz":-180},"id":"CrDHy6YRnmdr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"74985523-8f41-49af-ff9d-c9ea253578a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 24999 files belonging to 2 classes.\n","Using 17500 files for training.\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"id":"fb0fc5ee","metadata":{"id":"fb0fc5ee"},"outputs":[],"source":["# cat_files = os.listdir(os.path.join(dir, 'Cat'))\n","# dog_files = os.listdir(os.path.join(dir, 'Dog'))\n","\n","# # Split cat and dog files into train and temp (combined validation + test)\n","# cat_train, cat_temp = train_test_split(cat_files, test_size=0.3, random_state=42)\n","# dog_train, dog_temp = train_test_split(dog_files, test_size=0.3, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"id":"gKmt0Mh34E28","metadata":{"id":"gKmt0Mh34E28"},"outputs":[],"source":["# # Further split temp into validation and test\n","# cat_val, cat_test = train_test_split(cat_temp, test_size=0.5, random_state=42)\n","# dog_val, dog_test = train_test_split(dog_temp, test_size=0.5, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"id":"xgKaD45F6FSC","metadata":{"id":"xgKaD45F6FSC"},"outputs":[],"source":["# from PIL import Image\n","# import numpy as np\n","# import os\n","\n","# def load_images(file_list, base_dir, target_size=(128, 128)):\n","#     images = []\n","#     for file in file_list:\n","#         img_path = os.path.join(base_dir, file)\n","#         img = Image.open(img_path).resize(target_size)\n","#         img = np.array(img) / 255.0  # Normalize pixel values to [0, 1]\n","#         images.append(img)\n","#     return np.array(images)\n","\n","# # Base directories\n","# cat_dir = os.path.join(dir, 'Cat')\n","# dog_dir = os.path.join(dir, 'Dog')\n","\n","# # Load training images\n","# train_images = load_images(cat_train, cat_dir) + load_images(dog_train, dog_dir)\n","# train_labels = np.array([0] * len(cat_train) + [1] * len(dog_train))\n","# print('finish 1')\n","# # Load validation images\n","# val_images = load_images(cat_val, cat_dir) + load_images(dog_val, dog_dir)\n","# val_labels = np.array([0] * len(cat_val) + [1] * len(dog_val))\n","# print('finish 2')\n","# # Load test images\n","# test_images = load_images(cat_test, cat_dir) + load_images(dog_test, dog_dir)\n","# test_labels = np.array([0] * len(cat_test) + [1] * len(dog_test))\n","# print('finish 3')"]},{"cell_type":"markdown","id":"e2230c4b","metadata":{"id":"e2230c4b"},"source":["## Data Splitting\n","In this section, we will split our dataset into three parts:\n","\n","* Training set (70%): This portion of the dataset is used to train the CNN model.\n","* Validation set (15%): This portion is used to validate the model during training, helping us tune hyperparameters and avoid overfitting.\n","* Test set (15%): This portion is used to evaluate the model after training, to check its generalization to unseen data."]},{"cell_type":"code","execution_count":4,"id":"82f4df4b","metadata":{"executionInfo":{"elapsed":19560,"status":"ok","timestamp":1723984117138,"user":{"displayName":"Khaled","userId":"10230634685242631763"},"user_tz":-180},"id":"82f4df4b","colab":{"base_uri":"https://localhost:8080/"},"outputId":"24d0f2c0-3bfe-4aae-b7d8-955912c2cb1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 24999 files belonging to 2 classes.\n","Using 17500 files for training.\n","Found 24999 files belonging to 2 classes.\n","Using 7499 files for validation.\n"]}],"source":["\n","\n","# Load the dataset and split into training and a combined validation+test set\n","train_dataset = image_dataset_from_directory(\n","    dir,\n","    validation_split=0.3,  # Reserve 30% for validation + test (70% for training)\n","    subset=\"training\",\n","    seed=42,\n","    image_size=(64, 64),\n","    batch_size=32  # Optional: Set batch size here\n","\n",")\n","\n","validation_test_dataset = image_dataset_from_directory(\n","    dir,\n","    validation_split=0.3,  # Reserve 30% for validation + test\n","    subset=\"validation\",   # This subset will be further split into validation and test\n","    seed=42,\n","    image_size=(64, 64),\n","    batch_size=32\n",")\n","\n","# Calculate the number of batches in the combined validation+test dataset\n","val_test_batches = tf.data.experimental.cardinality(validation_test_dataset).numpy()\n","\n","# Further split the combined validation+test dataset into validation and test datasets\n","val_dataset = validation_test_dataset.take(val_test_batches // 2)  # 15% for validation\n","test_dataset = validation_test_dataset.skip(val_test_batches // 2) # 15% for testing\n"]},{"cell_type":"code","source":["# def normalize(image, label):\n","#     image = tf.cast(image, tf.float32) / 255.0  # Scale pixel values to [0, 1]\n","#     return image, label\n","\n","# train_dataset = train_dataset.map(normalize)\n","# val_dataset = val_dataset.map(normalize)\n","# test_dataset = test_dataset.map(normalize)\n"],"metadata":{"id":"wtUZ1-NYv90t"},"id":"wtUZ1-NYv90t","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8z6BWZdFwpst","executionInfo":{"status":"ok","timestamp":1723972721984,"user_tz":-180,"elapsed":12,"user":{"displayName":"Khaled","userId":"10230634685242631763"}},"outputId":"51d4879c-79dd-4061-cd79-31157aa3fe8a"},"id":"8z6BWZdFwpst","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_MapDataset element_spec=(TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","id":"fc3ce960","metadata":{"id":"fc3ce960"},"source":["## Building the CNN Model"]},{"cell_type":"markdown","id":"2ca1a1cb","metadata":{"id":"2ca1a1cb"},"source":["\n","Now, we will define our CNN architecture using `tensorflow.keras`. The architecture will consist of:\n","- Convolutional layers followed by max-pooling layers\n","- Flatten layer\n","- Dense layers\n","- Output layer\n"]},{"cell_type":"code","execution_count":null,"id":"91a5150c","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1723972747886,"user":{"displayName":"Khaled","userId":"10230634685242631763"},"user_tz":-180},"id":"91a5150c","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7b20c420-4b47-402b-c49d-6a516b2a60f0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}],"source":["model = Sequential([\n","    Conv2D(4, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n","    MaxPooling2D(2, 2),\n","    Conv2D(8, (3, 3), activation='relu'),\n","    MaxPooling2D(2, 2),\n","    # Conv2D(64, (3, 3), activation='relu'),\n","    # MaxPooling2D(2, 2),\n","    Flatten(),\n","    Dense(2, activation='relu'),\n","    Dense(4, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","id":"e0fff4b8","metadata":{"id":"e0fff4b8"},"source":["## Training the Model"]},{"cell_type":"markdown","id":"a84cb05c","metadata":{"id":"a84cb05c"},"source":["\n","Train the CNN model using the `fit` function. We will use the training and validation we created earlier.\n","\n","Fill in the code to train the model for a specified number of epochs.\n"]},{"cell_type":"code","execution_count":null,"id":"0e11ddcd","metadata":{"id":"0e11ddcd","executionInfo":{"status":"error","timestamp":1723973961924,"user_tz":-180,"elapsed":1139947,"user":{"displayName":"Khaled","userId":"10230634685242631763"}},"colab":{"base_uri":"https://localhost:8080/","height":499},"outputId":"1c2529b7-05e2-4c27-8641-ce2827d99c3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m298/547\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m15:52\u001b[0m 4s/step - accuracy: 0.5175 - loss: 0.6928"]},{"output_type":"error","ename":"InvalidArgumentError","evalue":"Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nNumber of channels inherent in the image must be 1, 3 or 4, was 2\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_2429]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-fe842a1e465d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nNumber of channels inherent in the image must be 1, 3 or 4, was 2\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_2429]"]}],"source":["history = model.fit(train_dataset, epochs=10, validation_data=val_dataset)"]},{"cell_type":"markdown","id":"3ba878f4","metadata":{"id":"3ba878f4"},"source":["## Evaluating the Model"]},{"cell_type":"markdown","id":"d508bce7","metadata":{"id":"d508bce7"},"source":["\n","After training, evaluate the model on the validation data to check its performance.\n"]},{"cell_type":"code","execution_count":null,"id":"5cd3a753","metadata":{"id":"5cd3a753"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"93e0c1b9","metadata":{"id":"93e0c1b9"},"source":["## Testing with New Images"]},{"cell_type":"markdown","id":"db94a8ae","metadata":{"id":"db94a8ae"},"source":["Finally, let's test the model with some new images. Preprocess the images and use the trained model to predict whether the image is of a cat or a dog.\n"]},{"cell_type":"code","execution_count":null,"id":"1cf379ec","metadata":{"id":"1cf379ec"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}